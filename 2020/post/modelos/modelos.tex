\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\input{../../../aux/tex/encabezado.tex}
\input{../../../aux/tex/tikzlibrarybayesnet.code.tex}

\usepackage{paracol}
%opening
\title{Reporte técnico}
\author{Gustavo Landfried}

\begin{document}

\maketitle

\begin{abstract}
 La inferencia Bayesiana es la forma \'optima de actualizar nuestras creencias dadas restricciones: modelo causal y datos.
\end{abstract}


\section{Un abordaje completo del espacio de hip\'otesis}

Un modelo \'optimo requiere de la evaluaci\'on completa del espacio de hip\'otesis siguiendo las reglas de razonamiento con incertidumbre.
 En esta secci\'on vamos a explicar cu\'ales son estas reglas y por qu\'e ellas son las \'unicas que garantizan actualizar de forma \'optima las creencias dado observables y modelos causales.
Afortunadamente estas reglas son intuitivas.
 
 Suponga que hay 3 cajas, solo una de ellas tiene lo que buscamos, pero no sabemos cu\'al ni tenemos m\'as informaci\'on.
 ¿Como repartirían ustedes su creencia?
 Una opción sería suponer que lo que buscamos está en la segunda caja.
 
\begin{figure}[H]     
    \centering \small
    \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \tikz{\node[factor, minimum size=1cm, xshift=-1.5cm] (p1) {} ;
        \node[factor, minimum size=1cm, xshift=0cm] (p2) {} ;
        \node[factor, minimum size=1cm, xshift=1.5cm] (p3) {} ;
         \node[const, above=of p1, yshift=.15cm] (fp1) {$10\%$};
         \node[const, above=of p2, yshift=.15cm] (fp2) {$80\%$};
         \node[const, above=of p3, yshift=.15cm] (fp3) {$10\%$};
        } 
    \end{subfigure}
\end{figure}

Pero si de verdad no sabemos d\'onde est\'a lo que buscamos, no parece ser esta la mejor forma de repartir nuestras creencias.
La mayor\'ia de las personas creen que no conviene concentrar la creencia en ninguna caja, sino repartirla en partes iguales.
Esta tendencia a maximizar la incertidumbre la vamos llamar honestidad.

\begin{figure}[H]     
    \centering \small
    \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \tikz{\node[factor, minimum size=1cm, xshift=-1.5cm] (p1) {} ;
        \node[factor, minimum size=1cm, xshift=0cm] (p2) {} ;
        \node[factor, minimum size=1cm, xshift=1.5cm] (p3) {} ;
        \node[const, above=of p1, yshift=.15cm] (fp1) {$33\%$};
        \node[const, above=of p2, yshift=.15cm] (fp2) {$33\%$};
        \node[const, above=of p3, yshift=.15cm] (fp3) {$33\%$};
    } 
    \end{subfigure}
\end{figure}

Si ahora sabemos que alguna de las cajas está vacía, ¿cómo deberíamos actualizar nuestras creencias?.
Si bien hay muchas formas de actualizarlas, la mayoría de las personas suelen pensar que conviene repartir las creencia en partes iguales entre las cajas restantes.

\begin{figure}[H]     
    \centering \small
    \begin{subfigure}[b]{0.49\textwidth}
    \centering
   \tikz{\node[factor, minimum size=1cm, xshift=-1.5cm] (p1) {} ;
        \node[det, minimum size=1cm, xshift=0cm] (p2) {} ;
        \node[factor, minimum size=1cm, xshift=1.5cm] (p3) {} ;
         \node[const, above=of p1, yshift=.15cm] (fp1) {$50\%$};
         \node[const, above=of p2, yshift=.15cm] (fp2) {$0\%$};
         \node[const, above=of p3, yshift=.15cm] (fp3) {$50\%$};
         } 
    \end{subfigure}    
\end{figure}

En este proceso adquirimos información de los datos.
Lo que ha demostrado Shannon~\cite{shannon1948-theoryOfCommunication} justamente es que la fuente de informaci\'on es la sorpresa, y que la cantidad de informaci\'on que extraemos de la sorpresa es

\begin{equation}
 \text{Informaci\'on}(x) = - \log(p(x))  
\end{equation}

Si supiéramos que un evento ocurrir\'a indefectiblemente $p(x)=1$, no recibiríamos información alguna, $\text{Informaci\'on}(x)=0$.
Si nos dijeran que un evento extremadamente improbable acaba de ocurrir $p(x) = 0$ estar\'iamos recibiendo ``infinita'' informaci\'on, $\text{Informaci\'on}(x) = \infty$.

Una propiedad fascinante es que aquello que llamamos honestidad, esa tendencia a maximizar la incertidumbre, es el comportamiento que nos garantiza maximizar la informaci\'on esperada, la que mejor nos abre al mundo.
\begin{equation}
 \text{Informaci\'on esperada}(X) = \sum_{x \in X} p(x) \, \text{Informaci\'on}(x) 
\end{equation}

La regla de la honestidad, entendida como la maximizaci\'on de la incertidumbre, va a ser la regla fundamental del razonamiento con incertidumbre~\cite{jaynes1957-informationTheoryAndStatisticalMechanics}.
Computar una creencia bajo la regla de la honestidad no va a ser m\'as que contar la cantidad de caminos en los que una hip\'otesis $A$ se hace verdadera sobre el total de caminos totales.

 \begin{equation*}
  \text{Creencia}(A) = \frac{\text{Cantidad de caminos en los que $A$ se hace verdadera}}{\text{Cantidad de caminos totales}}
 \end{equation*}
 
 \vspace{0.2cm}
 A partir de este principio se construye todo el razonamiento con incertidumbre.
 Descubr\'amoslo por nuestra cuenta a partir de un ejemplo.
 
 Supongamos que encontramos una cartuchera cerrada con 4 l\'apices.
 Solo sabemos que pueden ser o negros o blancos.
 ¿De que colores son los 4 l\'apices de la cartuchera?
 Sin m\'as informaci\'on, siguiendo el principio de honestidad no nos queda otra alternativa que repartir nuestra creencia en partes iguales.
 Ahora supongamos que vemos 3 veces a una persona sacar y guardar un l\'apiz.
 ¿Cuales son ahora nuestras creencias \'optimas?
 Para saberlo tenemos que contar caminos.

 \begin{figure}[H]     
    \centering \small
     \includegraphics[page=10,width=0.6\textwidth]{../../figures/forkingPath}
    
    \scriptsize Caminos totales (claros) y caminos posibles (oscuros) dada hip\'otesis \includegraphics[page=5,width=0.05\textwidth]{../../figures/forkingPath} y observaci\'on \includegraphics[page=1,width=0.043\textwidth]{../../figures/forkingPath}
    \caption{}
    \label{caminos}
\end{figure}

Tenemos en total 5 hip\'otesis $\{ \includegraphics[page=2,width=0.05\textwidth]{../../figures/forkingPath},\includegraphics[page=3,width=0.05\textwidth]{../../figures/forkingPath},\includegraphics[page=4,width=0.05\textwidth]{../../figures/forkingPath},\includegraphics[page=5,width=0.05\textwidth]{../../figures/forkingPath},\includegraphics[page=6,width=0.05\textwidth]{../../figures/forkingPath} \}$, 5 cartucheras posibles.
La cantidad de caminos que hacen verdadera a cada una de las hip\'otesis depende de la observaci\'on.
En la figura~\ref{caminos} vemos los caminos totales y posibles para una combinaci\'on de hip\'otesis \includegraphics[page=5,width=0.05\textwidth]{../../figures/forkingPath} y observaciones \includegraphics[page=1,width=0.043\textwidth]{../../figures/forkingPath}.
Cada combinaci\'on de hip\'otesis-observaci\'on tendr\'a una cantidad distinta de caminos posibles.
 
\begin{figure}[H]     
    \centering \small
  \begin{tabular}{ccccl}
                      &  \includegraphics[page=17,width=0.065\textwidth]{../../figures/forkingPath}                 & Observaci\'on $y_j$            &       \includegraphics[page=19,width=0.065\textwidth]{../../figures/forkingPath}                  &  \\[0.1cm] \cline{2-4}
\multicolumn{1}{r|}{\includegraphics[page=2,width=0.065\textwidth]{../../figures/forkingPath}} & \multicolumn{1}{c|}{\textcolor{white}{$n_{ij}$}} & \multicolumn{1}{r|}{} & \multicolumn{1}{l|}{} &  \\[0.1cm] \cline{2-4}
\multicolumn{1}{r|}{Hip\'otesis $x_i$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{\scriptsize Caminos posibles: \small $n_{ij}$} & \multicolumn{1}{l|}{} &  $\sum_j n_{ij} = {f_i}$ \\[0.1cm] \cline{2-4}
\multicolumn{1}{r|}{\includegraphics[page=6,width=0.065\textwidth]{../../figures/forkingPath}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{r|}{\textcolor{white}{$n_{ij}$}} &  \\[0.1cm] \cline{2-4}
&  & $\sum_i n_{ij} = {c_j}$ & & $N = \sum_j c_j = \sum_i f_i$
\end{tabular}

\end{figure}

 Habiendo contando caminos posibles y totales es que podemos computar cualquiera de las siguientes creencias.
 Todas ellas son honestas en el sentido de que maximizan la incertidumbre.

\begin{itemize} \setlength\itemsep{0.1cm}
 \item[$\ast$] \text{Creencia conjunta:} \ \ \  $C(X = x_i, Y= y_j) = \frac{n_{ij}}{N}$
 \item[$\ast$] \text{Creencia marginal:} \ \ \ $C(X = x_i) =  \frac{f_i}{N} = \sum_j C(X=x_i, Y= y_j) \color{white} = \frac{\sum_j n_{ij}}{N}$
 \item[$\ast$] \text{Creencia condicional:} \ \ \  $C(Y= y_j|X=x_i) = \frac{n_{ij}}{f_i} = \frac{n_{ij}}{N} \frac{N}{f_i} = \frac{C(X = x_i , Y = y_j)}{C(X = x_i)}$
\end{itemize}

Se ha logrado demostrar que estas reglas intuitivas son las \'unicas que permiten crear un razonamiento con incertidumbre que respete los principios b\'asicos: que permita representar las creencias con valores reales; que sea consistente, que cualquier razonamiento lleve a la misma conclusión; y que actualice las creencias en la direcci\'on del sentido com\'un~\cite{cox1946-reasonableExpectation}.
Estas creencias no son m\'as que las reglas de la probabilidad.


 \paragraph{Regla de la suma} Cualquier distribución marginal puede ser obtenida integrando la distribución conjunta.
 
\begin{equation*}
 P(X) = \sum_Y P(X,Y)
\end{equation*}
 
\paragraph{Regla del producto} Cualquier distribución conjunta puede ser expresada como el producto de distribuciones condicionales uni-dimensionles.

\begin{equation*}
 P(X,Y) = P(Y|X) P(X)
\end{equation*}

\paragraph{Actualizaci\'on de creencias} De las reglas de la suma y del producto se desprende la f\'ormula de actualizaci\'on de creencias, conocida tambi\'en como ``teorema Bayes''.
\begin{equation}
\begin{split}
 P(Y,X) &= P(X,Y) \\
  P(X|Y) P(Y) & = P(Y|X) P(X) \\
  P(X|Y)  & = \frac{P(Y|X) P(X)}{P(Y)}
 \end{split}
\end{equation}

Esta f\'ormula es la que nos permite actualizar de forma \'optima nuestras creencias sobre variables ocultas (hip\'otesis) a partir de observaciones (datos).
\begin{equation}
\underbrace{P(\text{Hip\'otesis }|\text{ Datos})}_{\text{\scriptsize Posteriori}} = \frac{\overbrace{P(\text{Datos }|\text{ Hip\'otesis})}^{\text{\scriptsize Verosimilitud}} \overbrace{P(\text{Hip\'otesis})}^{\text{\scriptsize Priori}} }{\underbrace{P(\text{Datos})}_{\text{\scriptsize Evidencia}}}
\end{equation}

En otras palabras, las creencias se actualizan en funci\'on de la sorpresa.
La verosimilitud no es otra cosa que la inversa de la sorpresa.
Cuando la sorpresa es total, la verosimilitud es nula, lo que act\'ua como filtro de las creencias previas.
\begin{equation}
 \text{Creencia de $h$ dado los datos} \propto (\,1-\text{Sorpresa del dato suponiendo $h$}\,) \cdot (\,\text{Creencia de $h$}\,)
\end{equation}

Usamos el s\'imbolo $\propto$ de proporcionalidad tan solo porque dejamos de dividir por la evidencia, que es el misma n\'umero para todas las hip\'otesis.

\section{Modelo lineal Bayesiano} \label{sec:lineal}

Cuando hablamos de modelos lineal nos referimos a funciones $\bm{t}= f(\bm{w},\bm{x}) + \epsilon $ que son lineales en sus parámetros $\bm{w}$, no en sus observables $\bm{x}$.
Utilizando transformaciones no lineales sobre los observables, $\bm{\phi}(\bm{x})$, es posible modelar cualquier tipo de relaci\'on no lineal entre los observables $\bm{x}$ y los no observables $\bm{t}$ mediante funciones lineales en los par\'ametros $\bm{w}$.

\vspace{0.3cm}

El modelo lineal es un modelo generativo. Propone la existencia de una relaci\'on causal $y(\cdot)$ entre una variable de inter\'es (target) $t$ y los vectores de variables observables $\bm{x}$ y no observables $\bm{w}$.
En t\'erminos generales la relaci\'on causal, $y(\cdot)$ se define como,

\begin{equation}
y(\bm{x},\bm{w}) = \sum_{i=0}^{M-1} w_i \phi_i(\bm{x}) = \bm{w}^T \bm{\phi}(\bm{x})
\end{equation}
 
Donde el vector $\bm{\phi}(\vm{x}_1) = ( \phi_0(\bm{x}_1), \, \phi_1(\bm{x}_1) , \, \dots , \, \phi_{M-1}(\bm{x}_1) )^T$ son la $M$ transformaciones no-lineales.
Notar que cada funci\'on de base $\phi_j(\cdot)$ recibe el vector-input completo $\bm{x}_i$.
Hoy trabajaremos con una \'unica dimensi\'on, por lo que el vector $\bm{x}_i$ ser\'a simplemente un escalar.
Aqu\'i usamos la convenci\'on $\phi_0(\bm{x})=1$.
El modelos lineal m\'as simple es aquel que tambi\'en es lineal en sus variables observables $\bm{x}$.
En este caso la transformaciones no es m\'as que la identidad $\bm{\phi}(\bm{x})=\bm{x}$.

Adem\'as de la parte estrictamente causal, el modelo lineal considera siempre la existencia de un factor aleatorio $\epsilon$.
El modelo completo que relaciona la variable de inter\'es $t$ con las variables observables $\bm{x}$ se compone de ambos t\'erminos.

\begin{equation}
 t = y(\vm{x},\vm{w}) + \epsilon
\end{equation}
 
El ruido aleatorio $\epsilon$ lo vamos a modelar proviniendo de una distribuci\'on Gaussiana, centrada en cero y precisi\'on (inversa de la varianza) $\beta$, $\epsilon \sim N(0,\beta^{-1})$.
\footnote{Una pregunta que surge naturalmente es plantearse si las decisiones que tomamos para modelar la relaciones entre variables es la correcta.En este punto no hay que perder de vista que los modelos no son m\'as que representaciones de la realidad, as\'i como los mapas no son el territorio.
Esto no significa que sean todas igualmente falsas y desechables.
Hay representaciones mejores que otras.
Y la inferencia Bayesiana prove\'e una forma de computar la creencias \'optimas sobre los modelos dada la evidencia.
Entonces terminemos de desarrollar la estructura de la familia de modelos lineales y dejemos la selecci\'on de modelos para despu\'es.}

Dado que tenemos una sola componente determinista $y(\cdot)$ y una sola componente aleatorio $\epsilon$, el modelo generativo de las variables de inter\'es $t$ es,
\begin{equation}
P(t | \bm{x}, \bm{w}, \beta) = \N(t | y(\bm{x},\bm{w}), \beta^{-1}) = \N(t | \bm{w}^T \bm{\phi}(\bm{x}) , \beta^{-1})
\end{equation}

% Planteado en t\'etminos gr\'aficos,
% 
% \begin{figure}[H]
%     \centering
%     \tikz{
% 
%     \node[latent, fill=black!100, minimum size=2pt] (x) {} ; %
%     \node[const, right=of x] (c_x) {$\vm{x}$};
%     \node[latent, fill=black!20, yshift=-1.5cm] (t) {$t$} ; %
%     \node[latent, fill=black!100, yshift=-1.5cm , xshift=-2cm,minimum size=2pt] (beta)
%     {} ; %
%     \node[const, above=of beta] (c_beta) {$\beta$};
%     \node[latent, fill=black!0, yshift=-1.5cm, xshift=2cm] (w) {$\vm{w}$};
%     \node[latent, fill=black!100, xshift=2cm, minimum size=2pt] (alpha) {} ; %
%     \node[const, right=of alpha] (c_alpha) {$\alpha$};
%     
%     \edge {x,beta,w} {t};
%     \edge {alpha} {w};
%     
%     \node[invisible, fill=black!0, minimum size=0pt, xshift=-0.52cm] (data_inv) {} ; %
%       
%     }  
% \end{figure}
% 

Generalmente se supone que las variables de inter\'es $t$ son independiente e id\'enticamente distribuida, por lo que la probabilidad conjunta de un vector de variables de inter\'es $\bm{t}$ se obtiene como la multiplicaci\'on de cada uno de los t\'erminos individuales

\begin{equation}
P(\bm{t} | \bm{x}, \bm{w}, \beta) = \prod_{i=1}^n \N(t_i | \bm{w}^T \bm{\phi}(\bm{x}_i) , \beta^{-1})
\end{equation}

% En t\'erminos gr\'aficos
% 
% 
% \begin{figure}[H]
%     \centering
%     \tikz{
% 
%     \node[latent, fill=black!100, minimum size=2pt] (x) {} ; %
%     \node[const, right=of x] (c_x) {$\vm{x}_n$};
%     \node[latent, fill=black!20, yshift=-1.5cm] (t) {$t_n$} ; %
%     \node[latent, fill=black!100, yshift=-1.5cm , xshift=-2cm,minimum size=2pt] (beta)
%     {} ; %
%     \node[const, above=of beta] (c_beta) {$\beta$};
%     \node[latent, fill=black!0, yshift=-1.5cm, xshift=2cm] (w) {$\vm{w}$};
%     \node[latent, fill=black!100, xshift=2cm, minimum size=2pt] (alpha) {} ; %
%     \node[const, right=of alpha] (c_alpha) {$\alpha$};
%     
%     \edge {x,beta,w} {t};
%     \edge {alpha} {w};
%     
%     \node[invisible, fill=black!0, minimum size=0pt, xshift=-0.52cm] (data_inv) {} ; %
%     \plate {data} {(t)(x)(c_x)(data_inv)} {$N$}; %
%       
%     }  
% \end{figure}

Tambi\'en podemos representar $N$ distribuciones Gaussianas independientes a trav\'es de una \'unica distribuci\'on Gaussiana multivariada, en la que la matriz de covarianzas solo tiene valores no nulos en la diagonal.

\begin{equation}
P(\bm{t} | \bm{x}, \bm{w}, \beta) = \prod_{i=1}^n \N(t_i | \bm{w}^T \bm{\phi}(\bm{x}_i) , \beta^{-1}) = \N(\bm{t}|\bm{w}^T \bm{\Phi}, \beta^{-1} \vm{I})
\end{equation}

Donde $\vm{I}$ es la matriz identidad y $\bm{\Phi}$ se la matriz de diseño

\begin{equation}
 \bm{\Phi} =
  \begin{pmatrix}
    \phi_0(\bm{x}_1) & \phi_1(\bm{x}_1) & \dots & \phi_{M-1}(\bm{x}_1)\\
    \vdots & \vdots & \ddots & \vdots \\
    \phi_0(\bm{x}_N) & \phi_1(\bm{x}_N) & \dots & \phi_{M-1}(\bm{x}_N)
  \end{pmatrix}
  = 
  \begin{pmatrix}
   \bm{\phi}(\vm{x}_1)^T \\
   \vdots \\
   \bm{\phi}(\vm{x}_N)^T \\
  \end{pmatrix}
\end{equation}


\subsection{Estimaci\'on puntual de par\'ametros}

La soluci\'on cl\'asica, o frecuentista, elige los par\'ametros $\bm{w}$ que tienen máxima verosimilitud.

\begin{equation}
 \bm{w}_\text{MV} = \underset{\bm{w}}{\text{ max }} p(\bm{t} | \bm{x}, \bm{w}, \beta)
\end{equation}

Los par\'ametros $\bm{w}$ que minimizan la distancia entre la curva $\bm{w}^T\bm{\phi}(\bm{x}_i)$ y los datos $\bm{t}$ son los que tienen mayor verosimilitud (demostraci\'on en el anexo).

\begin{equation}
 \underset{\bm{w}}{\text{ max }} P(\bm{t} | \bm{x}, \bm{w}, \beta) = \underset{\bm{w}}{\text{ min }} \sum_{i=1}^{n}  (t_i - \bm{w}^T\bm{\phi}(\bm{x}_i))^2 
\end{equation}

Aumentar la flexibilidad de la curvas $\bm{w}^T\bm{\phi}(\bm{x}_i)$ a trav\'es de modelos m\'as complejos, e.g. aumentando el grado de los modelos polinomiales, siempre permite reducir la distancia a los datos.
Este enfoque conduce a un problema conocido como over-fitting (o sobreajuste).
Las estrategias comunes para evitar el sobreajuste son, o agregar a la verosimilitud t\'erminos ad-hoc que penalicen la complejidad del modelo, o evaluando la verosimilitud en conjuntos de datos de validaci\'on distintos a los de entrenamiento.

\subsection{Estimaci\'on compelto del espacio de hipótesis}

Aqu\'i veremos como un tratamiento Bayesiano de la regresi\'on linea no solo evita el over-fitting que surge del criterio de estimaci\'on puntual basado en m\'axima verosimilitud, sino que ofrece una forma natural para seleccionar la complejidad del modelo usando tan solo los datos de entrenamiento.

\vspace{0.3cm}

Antes de computar nuestra distribuci\'on de creencias a posteriori sobre los par\'ametros $\bm{w}$ es necesario definir la distribuci\'on de creencias a priori.

\begin{equation}
 p(\vm{w}) = N(\vm{w}| \vm{0}, \alpha^{-1} \vm{I})
\end{equation}

Dada la elecci\'on de una distribuci\'on Gaussiana a priori, la distribuci\'on a posteriori tambi\'en ser\'a Gaussiana.
Esto surge de derivar de completar los cuadrados en el exponente y encontrar luego el coeficiente de normalizaci\'on.
El resultado general se encuentra en la ecuaci\'on (\ref{eq:post}) y es lo \'unico que se necesita para derivar la posteriori y la evidencia.
La distribuci\'on posteriori sobre $\vm{w}$ tiene como media

\begin{equation}
 \vm{m}_N = \beta  \vm{S}_N\vm{\Phi}^T \vm{t}
\end{equation}

y como covarianzas

\begin{equation}
 \vm{S}_N^{-1} = \alpha \vm{I} + \beta \vm{\Phi}^T\vm{\Phi}
\end{equation}

Esta soluci\'on anal\'itica considera a $\beta$ como constante conocida.
Y como en los problemas de regresi\'on no buscamos inferir la distribuci\'on del vector de entrada $\vm{x}$, este t\'ermino tambi\'en lo podemos tratar como constante conocida.
Luego, el modelo gr\'aficos es, 

\begin{figure}[H]
    \centering
    \tikz{

    \node[latent, fill=black!100, minimum size=2pt] (x) {} ; %
    \node[const, right=of x] (c_x) {$\vm{X}$};
    \node[latent, fill=black!20, yshift=-1.5cm] (t) {$\bm{t}$} ; %
    \node[latent, fill=black!100, yshift=-1.5cm , xshift=-2cm,minimum size=2pt] (beta)
    {} ; %
    \node[const, above=of beta] (c_beta) {$\beta$};
    \node[latent, fill=black!0, yshift=-1.5cm, xshift=2cm] (w) {$\vm{w}$};
    \node[latent, fill=black!100, xshift=2cm, minimum size=2pt] (alpha) {} ; %
    \node[const, right=of alpha] (c_alpha) {$\alpha$};
    
    \edge {x,beta,w} {t};
    \edge {alpha} {w};
    
    \node[invisible, fill=black!0, minimum size=0pt, xshift=-0.52cm] (data_inv) {} ; %
      
    }  
\end{figure}

Y la evidencia del modelo es

\begin{equation}
 P(t) = \N(\bm{t}| \vm{0}, \beta^{-1} \vm{I} + \alpha^{-1}\bm{\Phi}\bm{\Phi}^T  )
\end{equation}

A modo de visualizaci\'on, mostramos que ocurre con las creencias sobre la ordenada al origen y la pendiente de un modelo lineal a medida que van llegando datos.

 \begin{figure}[H]
\begin{subfigure}[t]{0.32\textwidth} 
\caption*{Verosimilitud} 
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\caption*{Priori/Posteriori} 
\includegraphics[width=\textwidth]{../../figures/linearRegression_posterior_0.pdf} 
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\caption*{Espacio de datos} 
\includegraphics[width=\textwidth]{../../figures/linearRegression_dataSpace_0.pdf} 
\end{subfigure}

\begin{subfigure}[c]{0.32\textwidth}
\includegraphics[width=\textwidth]{../../figures/linearRegression_likelihood_1.pdf} 
\end{subfigure}
\begin{subfigure}[c]{0.32\textwidth}
\includegraphics[width=\textwidth]{../../figures/linearRegression_posterior_1.pdf} 
\end{subfigure}
\begin{subfigure}[c]{0.32\textwidth}
\includegraphics[width=\textwidth]{../../figures/linearRegression_dataSpace_1.pdf} 
\end{subfigure}

\begin{subfigure}[c]{0.32\textwidth}
\includegraphics[width=\textwidth]{../../figures/linearRegression_likelihood_2.pdf} 
\end{subfigure}
\begin{subfigure}[c]{0.32\textwidth}
\includegraphics[width=\textwidth]{../../figures/linearRegression_posterior_2.pdf} 
\end{subfigure}
\begin{subfigure}[c]{0.32\textwidth}
\includegraphics[width=\textwidth]{../../figures/linearRegression_dataSpace_2.pdf} 
\end{subfigure}
\end{figure}

\subsection{Selecci\'on de modelo}\label{ssec:seleccion}

Cada vez que resolvemos a un problema de regresi\'on tenemos muchos modelos alternativos que podr\'iamos usar.
Para decidir sobre los distintos modelos, siempre es buena idea pensar en t\'erminos Bayesianos.
En este caso podemos computar cu\'al es nuestra distribuci\'on de creencias sobre los modelos dados los datos.

\begin{equation*}
 P(\text{M}|\text{D}) = \frac{P(\text{D}|\text{M})P(\text{M})}{ P(\text{D})}
\end{equation*}

En este caso no es posible calcular el denominador $P(\text{D})$, pero al comparar modelos este t\'ermino se cancela.

\begin{equation*}
  \frac{P(\text{M}_i|\text{D})}{P(\text{M}_j|\text{D})}  = \frac{P(\text{D}|\text{M}_i)\,P(\text{M}_i)}{ \underbrace{P(\text{D}|\text{M}_j)}_{\text{Evidencia!}}P(\text{M}_j)}  
\end{equation*}

Para comparar modelos solo necesitamos la predicci\'on a priori de los datos observados, es decir, su evidencia.
La predicci\'on a priori, al ser una distribuci\'on de probabilidad que integrar 1, sufre una penalizaci\'on natural a medida que se le agregan dimensiones a los modelos.

\begin{figure}[H]
\centering
  \includegraphics[width=0.6\textwidth]{../../figures/evidencia_de_modelos_alternativos} 
\end{figure}

Para verificar la utilidad de la evidencia (predicci\'on a priori de los datos observados), les proponemos realizar el siguiente experimento.

\begin{itemize}
  \item Generar datos de una sinoidal en el intervalo $[-0.5,0.5]$ con mucha precisi\'on $\beta^{-1} =25$
 \item Ajustar regresiones polinomiales de grado 0 hasta 9 con mucha incertidumbre a priori, ejemplo precisi\'on $\alpha^{-1}=10^-{7}$
\end{itemize}

\begin{figure}[H]
\centering
 \includegraphics[width=0.48\textwidth]{../../figures/model_selection_true_and_sample} 
 \includegraphics[width=0.48\textwidth]{../../figures/model_selection_MAP} 
\end{figure}

Luego,

\begin{itemize}
 \item Seleccionar modelo basado en la evidencia
 \item Comparar el comportamiento de la evidencia respecto al de m\'axima verosimilitud
\end{itemize}

\begin{figure}[H]     
     \centering 
     \begin{subfigure}[b]{0.47\textwidth}
       \includegraphics[width=1\textwidth]{../../figures/model_selection_evidence}
     \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
       \includegraphics[width=1\textwidth]{../../figures/model_selection_maxLikelihood}
     \end{subfigure}
\end{figure}






{\footnotesize
 \bibliographystyle{../../../aux/biblio/plos2015}
 \bibliography{../../../aux/biblio/biblio_notUrl}
}


\end{document}
